{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyserini integration\n",
    "\n",
    "Using pyserini to retrieve the top 1000 documents using BM25 and rank using ColBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATH = \"/home/kenzosaki/mestrado/data/pt_msmarco/indexes\"\n",
    "QUERIES_PATH = \"/home/kenzosaki/mestrado/data/pt_msmarco/google_translate/portuguese.queries.dev.small.tsv\"\n",
    "QRELS_PATH = \"/home/kenzosaki/mestrado/data/en_msmarco/data/msmarco_ans_small/qrels.dev.small.tsv\"\n",
    "# CHECKPOINT_PATH = \"/home/kenzosaki/mestrado/repos/ColBERT/experiments/ColBertimbau/train.py/colbertimbau/checkpoints/colbert-150000.dnn\"\n",
    "CHECKPOINT_PATH = \"/home/kenzosaki/mestrado/repos/ColBERT/experiments/ColBertimbau-linear_scheduler/train.py/colbertimbau-linear_scheduler/checkpoints/colbert-200000.dnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_PARAMETERS = {\n",
    "    \"k1\": 1.25, \n",
    "    \"b\": 0.74\n",
    "}\n",
    "\n",
    "RM3_PARAMETERS = {\n",
    "    \"fb_docs\": 10, \n",
    "    \"fb_terms\": 30, \n",
    "    \"original_query_weight\": 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLBERT_PARAMETERS = {\n",
    "    \"model_name\": \"neuralmind/bert-base-portuguese-cased\",\n",
    "    \"query_maxlen\": 32,\n",
    "    \"doc_maxlen\": 180,\n",
    "    \"dim\": 128,\n",
    "    \"similarity_metric\": \"cosine\",\n",
    "    \"mask_punctuation\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading indexes of MsMARCO-PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader\n",
    "from pyserini.analysis import  get_lucene_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reader = IndexReader(INDEX_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_terms': 335050083,\n",
       " 'documents': 8829003,\n",
       " 'non_empty_documents': 8829003,\n",
       " 'unique_terms': 1824588}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the BM25 retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = SimpleSearcher(INDEX_PATH)\n",
    "analyser = get_lucene_analyzer(\"pt\")\n",
    "searcher.set_analyzer(analyser) # para textos em portugues\n",
    "searcher.set_bm25(**BM25_PARAMETERS)\n",
    "searcher.set_rm3(**RM3_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"o que pode detectar o exame de urina\"\n",
    "hits = searcher.search(query, TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pode', 'detectar', 'exam', 'urin']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_reader.analyze(query, analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[tua, tenho, tínhamos, éramos, aos, isso, fora, minhas, seja, são, terá, hei, como, houver, tu, estiver, vocês, suas, até, estejam, tém, elas, tivéramos, está, tem, seríamos, for, terei, nas, esta, para, teu, isto, pela, estivéramos, hajamos, ao, hão, era, as, estivesse, estejamos, aquela, meus, temos, pelo, aquele, tivera, pelos, me, houveria, sua, estamos, estas, fôssemos, tiver, aquilo, dos, formos, estava, estiveram, sejamos, tivessem, qual, fosse, fossem, se, por, estivessem, houverei, tenham, aqueles, sem, estávamos, entre, mas, você, a, seu, e, houveram, será, houvesse, lhe, depois, houvéssemos, houvera, não, serei, o, seus, te, tivesse, tuas, houvemos, esteja, essas, seria, há, fui, tenhamos, essa, delas, houveríamos, esse, de, estivéssemos, teria, com, nossos, num, pelas, estivemos, do, esteve, da, no, estive, às, teriam, eles, das, houvessem, um, sou, ela, numa, quando, tiveram, ele, este, nossa, houveremos, teríamos, tivéssemos, estavam, em, havemos, tivemos, nosso, tinha, eu, tive, sejam, na, estes, foram, dela, já, houvermos, dele, tivermos, mesmo, mais, foi, que, houve, houverá, houvéramos, teremos, estiverem, uma, minha, quem, terão, nós, só, tinham, nos, nossas, eram, nem, tenha, serão, vos, seriam, meu, aquelas, hajam, à, teus, teve, os, ou, estivera, fomos, estou, forem, haja, estivermos, estão, esses, fôramos, também, houverão, somos, tiverem, deles, houveriam, muito, houverem, seremos, lhes]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: da pra adicionar mais stopwords aqui. do NLTK por ex?\n",
    "analyser.stopwordSet.toString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "queries_df = pd.read_csv(QUERIES_PATH, sep='\\t', header=None, names=['query_id', 'query'])\n",
    "qrels_df = pd.read_csv(QRELS_PATH, sep='\\t', header=None, names=['query_id', 'x', 'pid', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = queries_df.set_index('query_id').loc[qrels_df[\"query_id\"].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 105 entries, 352818 to 188714\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   query   105 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.6+ KB\n"
     ]
    }
   ],
   "source": [
    "queries_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352818</th>\n",
       "      <td>como cozinhar vagem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089760</th>\n",
       "      <td>a lesão ligamentar grave mais comum do joelho ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089312</th>\n",
       "      <td>tipos de fontes java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087904</th>\n",
       "      <td>com que idade as toupeiras aparecem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087589</th>\n",
       "      <td>quais são os gases usados ​​em sinais de néon?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      query\n",
       "query_id                                                   \n",
       "352818                                  como cozinhar vagem\n",
       "1089760   a lesão ligamentar grave mais comum do joelho ...\n",
       "1089312                                tipos de fontes java\n",
       "1087904                 com que idade as toupeiras aparecem\n",
       "1087589      quais são os gases usados ​​em sinais de néon?"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_bm25(queries_df: pd.DataFrame, searcher: SimpleSearcher, output_path: str, k: int = 1000) -> None:\n",
    "\n",
    "    # Output file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for query_id, row in tqdm(queries_df.iterrows(), total=len(queries_df) , desc=\"- Running BM25\"):\n",
    "            # BM25 retrieval\n",
    "            query = row[\"query\"]\n",
    "            hits = searcher.search(query, k)\n",
    "            \n",
    "            for rank, hit in enumerate(hits, start=1):\n",
    "                f.write(\"{}\\t{}\\t{}\\n\".format(query_id, hit.docid, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Running BM25: 100%|██████████| 105/105 [00:18<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "run_bm25(queries_df, searcher, \"data/runs/bm25_run.tsv\", k=TOPK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colbert.modeling.inference import ModelInference\n",
    "from colbert.modeling.colbert import ColBERT\n",
    "from colbert.utils.utils import load_checkpoint\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_colbert_from_checkpoint(path_to_checkpoint: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Carrega o modelo ColBERT a partir checkpoint.\n",
    "    \"\"\"\n",
    "    # Parametros usados para treino\n",
    "    colbert = ColBERT.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", **COLBERT_PARAMETERS)\n",
    "    colbert.to(device)\n",
    "\n",
    "    checkpoint = load_checkpoint(path_to_checkpoint, colbert, do_print=True)\n",
    "\n",
    "    colbert.eval()\n",
    "\n",
    "    return colbert, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing ColBERT: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ColBERT were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dez 20, 16:51:37] #> Loading checkpoint /home/kenzosaki/mestrado/repos/ColBERT/experiments/ColBertimbau-linear_scheduler/train.py/colbertimbau-linear_scheduler/checkpoints/colbert-200000.dnn ..\n",
      "[dez 20, 16:51:38] [WARNING] Loading checkpoint with strict=False\n",
      "[dez 20, 16:51:38] #> checkpoint['epoch'] = 0\n",
      "[dez 20, 16:51:38] #> checkpoint['batch'] = 200000\n",
      "Using neuralmind/bert-base-portuguese-cased tokenizer as the QueryTokenizer!\n",
      "Using neuralmind/bert-base-portuguese-cased tokenizer as the DocTokenizer!\n"
     ]
    }
   ],
   "source": [
    "# Preparing to inference\n",
    "colbert, checkpoint = load_colbert_from_checkpoint(CHECKPOINT_PATH, device)\n",
    "# colbert.to(device)\n",
    "inference_model = ModelInference(colbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_docs_and_pids_from_hits(hits):\n",
    "\n",
    "    docs = [hit.contents for hit in hits]\n",
    "    pids = [hit.docid for hit in hits]\n",
    "\n",
    "    return docs, pids\n",
    "\n",
    "def get_scores(inference_model, query, docs, bs):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Q = inference_model.queryFromText([query]).squeeze(0)\n",
    "        D_ = inference_model.docFromText(docs, bsize=bs)\n",
    "        scores = inference_model.colbert.score(Q, D_).cpu()\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def run_colbert(queries_df: pd.DataFrame,\n",
    "                searcher: SimpleSearcher,\n",
    "                inference_model: ModelInference, \n",
    "                output_path: str, \n",
    "                bs: int = 32,\n",
    "                k: int = 1000) -> None:\n",
    "\n",
    "    # Output file\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for query_id, row in tqdm(queries_df.iterrows(), total=len(queries_df) , desc=\"- Running ColBERT\"):\n",
    "            # BM25 retrieval\n",
    "            query = row[\"query\"]\n",
    "            hits = searcher.search(query, k)\n",
    "\n",
    "            docs, pids = get_docs_and_pids_from_hits(hits)\n",
    "            scores = get_scores(inference_model, query, docs, bs)\n",
    "\n",
    "            sorted_indexes = torch.argsort(scores, descending=True)\n",
    "            \n",
    "            for rank, index in enumerate(sorted_indexes, start=1):\n",
    "                f.write(\"{}\\t{}\\t{}\\n\".format(query_id, pids[index], rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'colbert-200000.dnn'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_name = CHECKPOINT_PATH.split(\"/\")[-1]\n",
    "checkpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- Running ColBERT: 100%|██████████| 105/105 [06:49<00:00,  3.90s/it]\n"
     ]
    }
   ],
   "source": [
    "run_colbert(queries_df, searcher, inference_model, f\"data/runs/{checkpoint_name}.linear_scheduler.run.tsv\", bs=512, k=TOPK)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00a652403d535cb12e9188c809d2a0719fc1987c230a4ef87caa4ec208dc5805"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('pygaggle': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
